<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# 决策树
`决策树可以理解为一种分支流程图`
## 3.1决策树的构造
- 优点：计算复杂度低，输出易于理解，能处理缺失数据，能处理不相关特征数据
- 缺点：可能产生过度匹配（过拟合？）问题
- 适用数据类型：数值型或标称型  

决策树的一般流程：  
1. 收集数据
2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
3. 分析数据
4. 训练算法：构造树的数据结构
5. 测试算法：使用经验树计算错误率
6. 使用算法



序号|不浮出水面能否生存|是否有脚蹼|属于鱼类  
:-:|:-:|:-:|:-:
1|是|是|是
2|是|是|是
3|是|否|否
4|否|是|否
5|否|是|否

### 3.1.1信息增益
- 信息增益 information gain
- 熵 entropy：代表集合的无序程度。    

---
信息的定义：如果待分类的事物可能划分在多个分类之中，则符号$$x_{i}$$的信息定义为：  
$$l(x_{i})=-\log_{2}p(x_{i})$$
其中$$p(x_{i})$$是选择该分类的概论。 
    
为了计算熵，我们需要计算所有类别可能值包含的信息期望值,通过以下公式
$$H=\sum_{i=1}^{n}p(x_{i})l(x_{i})$$  

---

程序清单3-1 计算给定数据集的香农熵
```PY
from math import log

def createDataSet():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no']]
    labels = ['no surfacing','flippers']
    #change to discrete values
    return dataSet, labels
    
def calcShannonEnt(dataSet):
    ent = 0
    m = len(dataSet)
    labelCount = {}
    for dataVec in dataSet:
        label = dataVec[-1]
        labelCount[label] = labelCount.get(label, 0) + 1
    for label in labelCount:
        probilaty = float(labelCount[label]) / m
        ent -= probilaty*log(probilaty, 2)
    return ent
```

### 3.1.2划分数据集
- 通过计算熵来判断划分结果好坏  
在trees.py中加入代码
```PY

```
