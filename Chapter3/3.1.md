<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
# 决策树
`决策树可以理解为一种分支流程图`
## 3.1决策树的构造
- 优点：计算复杂度低，输出易于理解，能处理缺失数据，能处理不相关特征数据
- 缺点：可能产生过度匹配（过拟合？）问题
- 适用数据类型：数值型或标称型  

决策树的一般流程：  
1. 收集数据
2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
3. 分析数据
4. 训练算法：构造树的数据结构
5. 测试算法：使用经验树计算错误率
6. 使用算法



序号|不浮出水面能否生存|是否有脚蹼|属于鱼类  
:-:|:-:|:-:|:-:
1|是|是|是
2|是|是|是
3|是|否|否
4|否|是|否
5|否|是|否

### 3.1.1信息增益
- 信息增益 information gain
- 熵 entropy：代表集合的无序程度。    

---
信息的定义：如果待分类的事物可能划分在多个分类之中，则符号$$x_{i}$$的信息定义为：  
$$l(x_{i})=-\log_{2}p(x_{i})$$
其中$$p(x_{i})$$是选择该分类的概论。 
    
为了计算熵，我们需要计算所有类别可能值包含的信息期望值,通过以下公式
$$H=\sum_{i=1}^{n}p(x_{i})l(x_{i})$$  

---

程序清单3-1 计算给定数据集的香农熵
```PY
from math import log

def createDataSet():
    dataSet = [[1, 1, 'yes'],
               [1, 1, 'yes'],
               [1, 0, 'no'],
               [0, 1, 'no'],
               [0, 1, 'no']]
    labels = ['no surfacing','flippers']
    #change to discrete values
    return dataSet, labels
    
def calcShannonEnt(dataSet):
    ent = 0
    m = len(dataSet)
    labelCount = {}
    for dataVec in dataSet:
        label = dataVec[-1]
        labelCount[label] = labelCount.get(label, 0) + 1
    for label in labelCount:
        probilaty = float(labelCount[label]) / m
        ent -= probility*log(probility, 2)
    return ent
```

### 3.1.2划分数据集
- 通过计算熵来判断划分结果好坏  
在trees.py中加入代码
```py
def splitDataSet(dataSet, axis, value):
    retMat = []
    for dataVec in dataSet:
        if dataVec[axis] == value:
            retMat.append(dataVec[:axis] + (dataVec[axis+1:]))
    return retMat

def chooseBestFeatureToSplit(dataSet):
    featureNum = len(dataSet[0]) - 1
    baseEntropy = calcShannonEnt(dataSet)
    bestInfoGain, bestFeature = 0.0, -1
    for i in range(featureNum):
        column_i = [dataVec[i] for dataVec in dataSet]
        uniqueVals = set(column_i)
        entropy = 0
        for val in uniqueVals:
            subMat = splitDataSet(dataSet, i, val)
            prob = len(subMat)/float(len(dataSet))
            entropy += prob * calcShannonEnt(subMat)
        #infoGain代表熵降， 熵降越多，分类越好
        infoGain = baseEntropy - entropy
        if infoGain > bestInfoGain:
            bestFeature = i
            bestInfoGain = infoGain
    return bestFeature  
```

### 3.1.3递归构建决策树

```py
# 当遍历完所有feature时，我们需要找到当前出现最多的类别
def majorCnt(classList):
    classCount = {}
    for vote in classList:
        classCount[vote] = classCount.get(vote, 0) + 1
    return max(classCount.iterkeys(), key=lambda vote:classCount[vote])

def createTree(dataSet, labels):
    """
    :type dataSet: np.array  
    :type labels: List  
    :rtype: labelName or tree
    """
    classList = [dataVec[-1] for dataVec in dataSet]
    # 类别完全相同，停止划分，返回类别
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # 遍历完所有特征，返回类别出现次数最多的
    if len(dataSet[0]) == 1:
        return majorCnt(classList)

    bestFeature = chooseBestFeatureToSplit(dataSet)
    bestFeatureLabel = labels[bestFeature]
    
    myTree = {bestFeatureLabel:{}}
    del(labels[bestFeature])

    featureVals = [dataVec[bestFeature] for dataVec in dataSet]
    uniqueVal = set(featureVals)
    
    for val in uniqueVal:
        # 传递labels的拷贝给递归函数
        subLabels = labels[:]
        myTree[bestFeatureLabel][val] = createTree(splitDataSet(dataSet, bestFeature, val), subLabels)
    return myTree
```
